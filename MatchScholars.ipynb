{
 "cells": [
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra, Statistics\n",
    "using TextAnalysis, ExcelReaders, StringDistances, AMD, SparseArrays, Plots\n",
    "cd(@__DIR__)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This script analyzes the abstracts published by the department of automatic control, Lund University, and looks for similarities amngost the abstracts of the visiting scholars of the LCCC focus period\n",
    "The department abstract and authornames were acquired from https://lup.lub.lu.se/search/publication?q=department+exact+v1000253 and exported as an excel file [publications.xsl](https://github.com/baggepinnen/MatchScholars.jl/blob/master/publications.xsl). The abstracts of the visiting scolars are provided in [visitors.txt](https://github.com/baggepinnen/MatchScholars.jl/blob/master/visitors.txt)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Read department data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "filename  = \"publications.xls\"\n",
    "authors   = readxl(filename, \"Sheet1!E2:E1501\")[:]\n",
    "abstracts = readxl(filename, \"Sheet1!X2:X1501\")[:];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Filter the data to only keep valid names and abstracts"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "valid_authors   = isa.(authors,String)\n",
    "authors         = authors[valid_authors]\n",
    "abstracts       = abstracts[valid_authors]\n",
    "\n",
    "valid_abstracts = isa.(abstracts,String)\n",
    "authors         = authors[valid_abstracts]\n",
    "abstracts       = abstracts[valid_abstracts]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Some abstracts are in Swedish, we get rid of those"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "valid_abstracts = [match(r\"ENG\", a) != nothing && match(r\"och\", a) == nothing for a in abstracts]\n",
    "authors         = authors[valid_abstracts]\n",
    "abstracts       = abstracts[valid_abstracts]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Clean data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "authors   = [replace(a, r\"\\([\\w-]+\\)\" => \"\") for a in authors]\n",
    "abstracts = [replace(a, r\"\\([\\w-]+\\)\" => \"\") for a in abstracts]\n",
    "abstracts = [replace(a, r\"ENG:\"       => \"\") for a in abstracts]\n",
    "@assert length(authors) == length(abstracts)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Read visitor data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "visitortext = readlines(\"visitors.txt\")\n",
    "visitors = map(visitortext) do v\n",
    "    m = match(r\"==name== (.+)$\", v)\n",
    "    if m == nothing\n",
    "        return m\n",
    "    end\n",
    "    String(m.captures[1])\n",
    "end\n",
    "visitors = String.(filter!(x->x!=nothing, visitors))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Assemble the abstract from individual lines"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function build_abstract(lines, abstract=\"\", abstracts=String[])\n",
    "    if length(lines) < 1\n",
    "        push!(abstracts, abstract)\n",
    "        return abstracts\n",
    "    end\n",
    "    if match(r\"==name==\", lines[1]) != nothing # Found new name\n",
    "        push!(abstracts, abstract)\n",
    "        return build_abstract(lines[2:end], \"\", abstracts)\n",
    "    end\n",
    "    return build_abstract(lines[2:end], abstract*\" \"*lines[1], abstracts)\n",
    "end\n",
    "visitor_abstracts = build_abstract(visitortext)[2:end]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Prepare data for text analysis"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "docs = StringDocument.([deepcopy(abstracts); deepcopy(visitor_abstracts)])\n",
    "crps = Corpus(deepcopy(docs))\n",
    "prepare!(crps, strip_corrupt_utf8 | strip_case | strip_articles | strip_prepositions | strip_pronouns | strip_stopwords | strip_whitespace | strip_non_letters | strip_numbers)\n",
    "remove_words!(crps, [\"br\", \"control\", \"system\", \"systems\"]) # For some reason the word \"br\" appeas very often (html tag?)\n",
    "update_lexicon!(crps)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Analysis\n",
    "We will perform two sets of analysis, [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) and [Latent Semantic Analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "Ideally, since we are going to use the founds topics for similarity analysis, we should use a correlated topic model (CTM). I could not find a working implementation of that and didn't have the time to fix one, so LDA will have to do. We can estimate topic correlations adhoc using either ϕ*ϕ' (on a word similarity basis) or θ*θ' (on author similarity basis)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "LDA"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m     = DocumentTermMatrix(crps)\n",
    "k     = 6    # number of topics\n",
    "iters = 1000 # number of gibbs sampling iterss\n",
    "α     = 1/k  # hyper parameter topics per document\n",
    "β     = 0.01 # hyper parameter words per topic\n",
    "ϕ,θ   = lda(m, k, iters, α, β) # ϕ: topics × words θ: topics × documents\n",
    "println(\"Occupancy: \", sum(ϕ.!=0)/length(ϕ))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Calculate a topic matrix if you want to inspect the found topics"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "k_largest(array,k) = sortperm(array, rev=true)[1:k]\n",
    "words_per_topic = 20\n",
    "topics = map(1:k) do topic_num\n",
    "    probs = Vector(ϕ[topic_num,:])\n",
    "    inds_of_largest = k_largest(probs,words_per_topic)\n",
    "    words = m.terms[inds_of_largest]\n",
    "    words\n",
    "end\n",
    "topics = hcat(topics...)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "`topics` is now a matrix where each column consists of the 20 most prominent words in each topic\n",
    "We can also define some interesting covaiance matrices for visualization (plots omitted)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function corr(x::AbstractMatrix)\n",
    "    d = diag(x)\n",
    "    y = copy(x)\n",
    "    for i = 1:length(d), j = 1:length(d)\n",
    "        y[i,j] /= sqrt(x[i,i]*x[j,j])\n",
    "    end\n",
    "    y\n",
    "end\n",
    "topic_covariance_by_words     = Matrix(ϕ*ϕ')\n",
    "topic_covariance_by_documents = Matrix(θ*θ')\n",
    "topic_correlation_by_words = corr(topic_covariance_by_words)\n",
    "topic_correlation_by_documents = corr(topic_covariance_by_documents)\n",
    "document_covariance           = θ'θ"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We now conduct some tedious coding to associate the authors of the abstracts in the publication database with the current staff members"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "individual_authors = String.(strip.(vcat(split.(authors, \";\")...)))\n",
    "unique_authors     = unique(individual_authors)\n",
    "author_mapping = map(unique_authors) do ua\n",
    "    [match(Regex(ua), aa) != nothing for aa in authors]\n",
    "end\n",
    "author_mapping = findall.(author_mapping)\n",
    "author_rank    = sortperm(length.(author_mapping), rev=true)\n",
    "top_10_authors = unique_authors[author_rank[1:10]]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The current staff at the department is listed in [staff.txt](staff.txt)\n",
    "Since the names of the authors are not on the same format in the database and on the webpage, we use a [string comparison tool](https://github.com/matthieugomez/StringDistances.jl) to find correspondences.\n",
    "\"The distance Tokenmax(RatcliffObershelp()) is a good choice to link names or adresses across datasets.\""
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "staff = readlines(\"staff.txt\")\n",
    "\n",
    "staff2unique_author = map(staff) do staffi\n",
    "    distances = map(unique_authors) do authori\n",
    "        StringDistances.compare(TokenMax(RatcliffObershelp()), filter(isascii,authori), filter(isascii,staffi))\n",
    "    end\n",
    "    max_i = argmax(distances)\n",
    "    distances[max_i] < 0.9 && return 0\n",
    "    return max_i\n",
    "end\n",
    "filter!(!iszero, staff2unique_author)\n",
    "staff_authorname = unique_authors[staff2unique_author]\n",
    "\n",
    "document_indices_of_staff = map(staff_authorname) do staffi\n",
    "    r = Regex(staffi)\n",
    "    present = map(authors) do authori\n",
    "        match(r, authori) != nothing\n",
    "    end\n",
    "    findall(present)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "To find the topic vectors of each staff members, we average over all their publications. This is a simple way of doing things, but might cause a senior author with a diverse set of publications to appear as having little similarity with a young researcher with a narrow focus, even if the senior author has a few publications in that particular topic."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "staff_vectors = map(document_indices_of_staff) do staffdocinds\n",
    "    # ϕ: topics × words   θ: topics × documents\n",
    "    mean(θ[:,staffdocinds], dims=2)\n",
    "end\n",
    "staff_vectors = hcat(staff_vectors...) # n_topics × n_staff"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The vectors of the visiting scholars are easy to find since they have only a single abstract each"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "visitor_vectors = θ[:,end-length(visitors)+1:end] # n_topics × n_visitors"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The correlation between the abstracts of the staff and the abstracts of the visiting scholars are given by the inner product of their respective topic vectors"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "visitor_staff_covariance = staff_vectors'topic_correlation_by_documents*visitor_vectors"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before we plot the covariance matrix, we try to approximately diagonalize it using the [AMD algorithm](https://github.com/JuliaSmoothOptimizers/AMD.jl). To do this, we have to set some elements that fall beneath a certain threshold to zero. We plot a histogram to assist us in setting this threshold"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function diagonalize(C, tol; permute_y=false, doplot=true)\n",
    "    C = copy(C)\n",
    "    amdmat = size(C,1) == size(C,2) ? copy(C) : C'C\n",
    "    # amdmat = C'C\n",
    "    doplot && (histogram(abs.(amdmat[:])) |> display)\n",
    "    amdmat[abs.(amdmat) .< tol] .= 0\n",
    "    permutation = amd(sparse(amdmat))\n",
    "    ypermutation = permute_y ? permutation : 1:size(C,1)\n",
    "    C[ypermutation,permutation], permutation, ypermutation\n",
    "end\n",
    "function plotcovariance(C, xvector, yvector; kwargs...)\n",
    "    xticks = (1:length(xvector), xvector)\n",
    "    yticks = (1:length(yvector), yvector)\n",
    "    heatmap(C; xticks=xticks, yticks=yticks, xrotation=90, title=\"Author similarity\", kwargs...)\n",
    "end\n",
    "\n",
    "C, permutation, ypermutation = diagonalize(visitor_staff_covariance, 4, permute_y=false)\n",
    "plotcovariance(C, visitors[permutation], staff_authorname[ypermutation], xlabel=\"Visiting scholars\", ylabel=\"Control staff\", size=(600,1000))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We can do the same analysis among the staff members"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "staff_covariance = staff_vectors'topic_correlation_by_documents*staff_vectors\n",
    "\n",
    "C, permutation, ypermutation = diagonalize(staff_covariance, 0.3, permute_y=true, doplot=true)\n",
    "plotcovariance(C,staff_authorname[permutation],staff_authorname[permutation], xrotation=90, size=(1000,1000), yflip=true)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "To compile this page, run `Literate.notebook(\"MatchScholars.jl\", \".\", documenter=false, execute=false, credit=false); convert_doc(\"MatchScholars.ipynb\", \"MatchScholars.jmd\"); weave(\"MatchScholars.jmd\")`"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  },
  "kernelspec": {
   "name": "julia-1.0",
   "display_name": "Julia 1.0.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
